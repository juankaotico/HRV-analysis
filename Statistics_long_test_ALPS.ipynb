{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11383a0a-8d7b-420f-a3a4-cbe83ef7170f",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a72d5-abd4-4022-9830-3f2f97b90faa",
   "metadata": {},
   "source": [
    "**Instalation of the packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb38691-d9d4-484e-80f2-1ad37ce99c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"statsmodels\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3766908-4502-418a-ae76-13a4180db0e7",
   "metadata": {},
   "source": [
    "## Read and merge files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d6e6d-0841-4e48-969a-4b4a04c8f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import statistics\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd11c1f-aaab-45ae-9285-6b5ea53ed4a2",
   "metadata": {},
   "source": [
    "## Read and prepare the files from ALPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad90fd5-9f3f-4276-8a41-81d119f46b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(r\"C:\\Users\\juank\\OneDrive\\Desktop\\Master_code\\ALPS\\HRV\\Sub1\\*.csv\")\n",
    "\n",
    "Sub1 = pd.DataFrame()\n",
    "for f in files:\n",
    "    csv = pd.read_csv(f)\n",
    "    csv[\"Unit\"] = csv[\"Unit\"].fillna(\" \")\n",
    "    csv[\"Variables\"] = csv[\"Variable\"] + \" \" + csv[\"Unit\"]\n",
    "    Label = csv.Label[1]\n",
    "    File = \"FileName\"\n",
    "    csv.rename(columns={\"Value\":Label}, inplace= True)\n",
    "    csv = csv.drop([\"Signal\",\"Method\",\"Unit\",\"Variable\",\"Session\",\"Dataset\",\"Label\",\"Subject\"],axis=1)\n",
    "    csv = csv.set_index(\"Variables\")\n",
    "    csv = csv.T\n",
    "    Sub1 = Sub1.append(csv)\n",
    "#merge all the files created in ALPS per subject into only one df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6453fefa-1fdc-44c4-9815-d58b265acba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sub1.index.names = [\"FileName\"] #set an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9af8df-351c-40ed-a973-cc512905f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(r\"C:\\Users\\juank\\OneDrive\\Desktop\\Master_code\\ALPS\\HRV\\Sub2\\*.csv\")\n",
    "\n",
    "Sub2 = pd.DataFrame()\n",
    "for f in files:\n",
    "    csv = pd.read_csv(f)\n",
    "    csv[\"Unit\"] = csv[\"Unit\"].fillna(\" \")\n",
    "    csv[\"Variables\"] = csv[\"Variable\"] + \" \" + csv[\"Unit\"]\n",
    "    Label = csv.Label[1]\n",
    "    File = \"FileName\"\n",
    "    csv.rename(columns={\"Value\":Label}, inplace= True)\n",
    "    csv = csv.drop([\"Signal\",\"Method\",\"Unit\",\"Variable\",\"Session\",\"Dataset\",\"Label\",\"Subject\"],axis=1)\n",
    "    csv = csv.set_index(\"Variables\")\n",
    "    csv = csv.T\n",
    "    Sub2 = Sub2.append(csv)\n",
    "#merge all the files created in ALPS per subject into only one df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82218ca8-6980-44cb-957e-295acaf67755",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sub2.index.names = [\"FileName\"] #set an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e33e4-b826-411d-b4fc-31472e08a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ALPS = pd.concat([Sub1,Sub2,Sub3,Sub4], axis = 0) # merge al dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89961d-5a14-47ad-9c52-c8d65a5d7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ALPS #visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb6ffa-e35e-4de2-92eb-f59d15da875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ALPS = df_ALPS.set_axis(['S1_VLFpeak_FFT Hz', 'S1_VLFpow_FFT ms2', 'S1_VLFpow_FFT log', \"S1_VLFpow_FFT %\",\n",
    "                 \"S1_LFpeak_FFT Hz\",'S1_LFpow_FFT ms2','S1_LFpow_FFT log','S1_LFpow_FFT %','S1_LFpow_FFT n.u.',\n",
    "                  'S1_HFpeak_FFT Hz','S1_HFpow_FFT ms2','S1_HFpow_FFT log','S1_HFpow_FFT %','S1_HFpow_FFT n.u.',\n",
    "                  'S1_LF_HF_ratio_FFT','S1_SampEn','S1_Mean RR ms','S1_Mean HR bpm','S1_Max HR bpm','S1_Min HR bpm',\n",
    "                 'S1_SD HR bpm','S1_SDNN ms','S1_SDNNI ms','S1_SDANN ms','S1_RMSSD ms','S1_NNxx beats',\n",
    "                  'S1_pNNxx %', 'S1_HRV triangular index','S1_TINN ms'], axis='columns')\n",
    "#change the axis of the df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee7b48-7214-41d9-a804-ba1428c8c7e6",
   "metadata": {},
   "source": [
    "## Read and prepare the files from KUBIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e2aa2-e418-4d2a-8956-c5bc040d8900",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sub1k = pd.read_csv(\"path...\\KubiosHRVresultsSub1.csv\", header = 1) #read data file\n",
    "Sub2k = pd.read_csv(\"path...\\KubiosHRVresultsSub2.csv\", header = 1) #read data file\n",
    "Sub3k = pd.read_csv(\"path...\\KubiosHRVresultsSub3.csv\", header = 1) #read data file\n",
    "#... add all files from Kubios HRV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf311ae1-8d5a-4afa-bca8-1abed7abfe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kubios = pd.concat([Sub1k,Sub2k,Sub3k,Sub4k], axis = 0) # merge all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b2bca-be18-49d1-9462-2a66bfee9fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "df_kubios = df_kubios.drop(df_kubios.columns[cols],axis=1) # eliminate informational columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8759bab-4212-4ccb-82be-ec4bc170b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kubios.drop(columns=[\"Unnamed: 103\"], inplace=True) # eliminate \"random\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d15df2-9b72-40cb-912b-5343bd7e3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [1,2,3,31,33, 34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,55,56,57,58,59,60,61,62,63,64,65,\n",
    "        66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82]\n",
    "df_kubios = df_kubios.drop(df_kubios.columns[cols],axis=1) # eliminate informational columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37532f-852b-4971-a47d-db0e0d55f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kubios = df_kubios.set_index(\"FileName\") #set the ne indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1226f-7b42-4e6d-9934-1f43f8c40db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kubios = df_kubios.filter(like = \"Empatica\", axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05784c0a-38b2-4cf4-ab6a-3258c998c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kubios = df_kubios.astype(float)\n",
    "df_kubios.rename(columns=lambda x: x.replace('(',''), inplace=True)\n",
    "df_kubios.rename(columns=lambda x: x.replace(')',''), inplace=True)  #delete RE expresions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b434ec-3754-42d0-a684-caa22bc9e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kubios #visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da9a0f-4658-4365-a35a-c6c0e4aebac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_kubios.astype(float)\n",
    "df2 = df_ALPS.astype(float) # transform all our values to floats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021c72bc-f639-443c-be30-d4050845a68e",
   "metadata": {},
   "source": [
    "## Alter the label of our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f24b5c-b6db-4503-8f8c-13f9ea16784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rename(index=lambda x: x.replace('Sub1_Empatica_Segment','1_'), inplace=True)\n",
    "df1.rename(index=lambda x: x.replace('Sub2_Empatica_Segment','2_'), inplace=True)\n",
    "df1.rename(index=lambda x: x.replace('Sub3_Empatica_Segment','3_'), inplace=True)\n",
    "\n",
    "df1.rename(index=lambda x: x.replace('.csv',''), inplace=True)\n",
    "# edit the beginning of the index of our df's, depends of how was save in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a0848-3b47-493e-be71-3309e9204938",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 #visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0dacb4-31f3-431f-9e3c-affe052a2a58",
   "metadata": {},
   "source": [
    "## Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019dd6bd-f416-4d52-bb62-6fde200daba4",
   "metadata": {},
   "source": [
    "### Mean Difference - Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae18c26c-da89-4319-a3ab-81982a0a90d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = pd.Index(df_kubios.columns) #create index for iteration process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d2d91-f125-49cf-bbd6-89836e219020",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data1 = []\n",
    "data2 = [] #create empty lists to save the data\n",
    "for column in df_kubios.columns: #mean difference between the complete datasets\n",
    "        x = df_kubios[column].astype(float).mean() #list the means of Empatica\n",
    "        y = df_ALPS[column].astype(float).mean() #list the means of Faros\n",
    "        z = (df_kubios[column].astype(float).mean() - df_ALPS[column].astype(float).mean()) #list the difference of the means\n",
    "        z = abs (z)\n",
    "        data.append (x)\n",
    "        data1.append (y)\n",
    "        data2.append (z)\n",
    "mean_x = pd.DataFrame (data, index = intro, columns=[\"Means Kubios\"])\n",
    "mean_y = pd.DataFrame (data1, index = intro, columns=[\"Means ALPS\"])\n",
    "mean_z = pd.DataFrame (data2, index = intro, columns=[\"Mean Difference\"])\n",
    "#generate df's\n",
    "mean_diff = pd.concat ([mean_x,mean_y,mean_z], axis = 1) #concat our 3 datasets\n",
    "mean_diff.index.name = \"Name\" #give name to the index\n",
    "mean_diff#visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb9b68f-8bc0-4920-91df-8b59d459a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 #visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a7fa3-c80e-44d5-952a-90259d1a4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 #visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1004f95-7a25-407a-b17d-22085b4cdb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df12_merged = df1.merge(df2, suffixes=[\" _kubios\", \" _ALPS\"], on=\"FileName\") # merge the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbb3a79-5e87-4b02-a71d-3936c7467bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df12_merged #visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825621c7-0e78-4b27-b1b0-b4c6a03ca24c",
   "metadata": {},
   "source": [
    "Fix the names of our columns (remove \"(\" and \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541a2a4-ba8a-4cdc-9fe5-0b4c2eb7cb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df12_merged.rename(columns=lambda x: x.replace('(',''), inplace=True)\n",
    "df12_merged.rename(columns=lambda x: x.replace(')',''), inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8324c2d0-ae65-4a38-91de-c8ea83ed9633",
   "metadata": {},
   "source": [
    "Create our working list for itemize in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c03ac-bcc6-449a-a95e-209a7bca0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = df_Emp #copy one of the original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b070b-44ae-4f20-a6fe-d0e306efdc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.rename(columns=lambda x: x.replace('(',''), inplace=True)\n",
    "w.rename(columns=lambda x: x.replace(')',''), inplace=True) #remove the \"(\" and \")\" 's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca7630-d0c4-430b-916e-1118831568e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = w.columns # create a list for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b74ef2-004e-4102-93a9-9731f4493a0c",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9127f0-0ab1-4f16-883d-afbf4d510afc",
   "metadata": {},
   "source": [
    "We want to explore the distribution of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac26646-f2bb-47b1-951a-bd4cd835bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_emp = pd.DataFrame ((df1.loc[ : , df1.columns ].skew(axis = 0, skipna = True)), columns=[\"Skew_kubios\"])\n",
    "skew_emp.index.name = \"HRV metric\" #give name to the index\n",
    "skew_far = pd.DataFrame ((df2.loc[ : , df2.columns ].skew(axis = 0, skipna = True)), columns = [\"Skew_ALPS\"])\n",
    "skew_far.index.name = \"HRV metric\" #give name to the index\n",
    "skew_tot = skew_emp.merge(skew_far, on = \"HRV metric\")\n",
    "skew_tot #skewness of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a220333c-000a-4cf8-904e-73e3997d2cc9",
   "metadata": {},
   "source": [
    "If the skewness is between -0.5 and 0.5, the data are fairly symmetrical\n",
    "If the skewness is between -1 and â€“ 0.5 or between 0.5 and 1, the data are moderately skewed\n",
    "If the skewness is less than -1 or greater than 1, the data are highly skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca0ffc-e3e1-481e-bf22-92c809a3ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_tot.loc[ abs(skew_tot[\"Skew_kubios\"]) > 0.5] # number of values moderately skewed for Kubios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaee566-3846-4735-af2a-fd5d7d2f506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_tot.loc[ abs(skew_tot[\"Skew_kubios\"]) > 1] # number of values highly skewed for Kubios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e1540-9656-4015-a915-cf91771dcd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_tot.loc[ abs(skew_tot[\"Skew_ALPS\"]) > 0.5] # number of values moderately skewed for ALPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254a734-0952-4100-b15c-56c049cd7a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_tot.loc[ abs(skew_tot[\"Skew_ALPS\"]) > 1] # number of values highly skewed for ALPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e4d2a-fcfb-41f7-a9d2-00a7ab592227",
   "metadata": {},
   "source": [
    "### Evaluate the normallity of our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe550c-c67d-457e-850e-ee8dc2022e18",
   "metadata": {},
   "source": [
    "We can also explore the distribution with Shapiro-Wilk test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350307c2-85a5-46f5-99ce-c6296050ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap1 = []\n",
    "s_val_p1 = []\n",
    "colnew_s1 =[] #create empty lists to save the data\n",
    "for column in df1.columns: #iterate in our reference list\n",
    "        x = df1.filter(like=column) #filter from the combine dataset only the columns that start with the \"name\" at a time\n",
    "        x = x.dropna() #drop the NaN for the specified duplet\n",
    "        if len(x) > 2: # setting a threshold of at least 3 values\n",
    "            r, p = stats.shapiro(x) #shapiro test\n",
    "            shap1.append (r)\n",
    "            s_val_p1.append (p)\n",
    "            colnew_s1.append (column) # add the specified values to the list\n",
    "res_shap1 = pd.DataFrame (shap1, columns=[\"Shap_statistics_kubios\"])\n",
    "res_shap2 = pd.DataFrame (s_val_p1, columns=[\"P-value(Shap_stats_kubios)\"])\n",
    "res_shap3 = pd.DataFrame (colnew_s1, columns=[\"Name\"]) # create df from lists\n",
    "shap_stats1 = pd.concat([res_shap1,res_shap2,res_shap3], axis =1) #merge into one\n",
    "\n",
    "\n",
    "shap2 = []\n",
    "s_val_p2 = []\n",
    "colnew_s2 =[] #create empty lists to save the data\n",
    "for column in df1.columns: #iterate in our reference list\n",
    "        x = df2.filter(like=column) #filter from the combine dataset only the columns that start with the \"name\" at a time\n",
    "        x = x.dropna() #drop the NaN for the specified duplet\n",
    "        if len(x) > 2: # setting a threshold of at least 3 values\n",
    "            r, p = stats.shapiro(x) #shapiro test\n",
    "            shap2.append (r)\n",
    "            s_val_p2.append (p)\n",
    "            colnew_s2.append (column) # add the specified values to the list\n",
    "res_shap4 = pd.DataFrame (shap2, columns=[\"Shap_statistics_ALPS\"])\n",
    "res_shap5 = pd.DataFrame (s_val_p2, columns=[\"P-value(Shap_stats_ALPS)\"])\n",
    "res_shap6 = pd.DataFrame (colnew_s2, columns=[\"Name\"]) # create df from lists\n",
    "shap_stats2 = pd.concat([res_shap4,res_shap5,res_shap6], axis =1) #merge into one\n",
    "\n",
    "\n",
    "shap_stats_tot = pd.merge(shap_stats1, shap_stats2, how= \"outer\", on=\"Name\") \n",
    "shap_stats_tot.set_index(\"Name\", inplace = True)\n",
    "shap_stats_tot\n",
    " #visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25477dc9-bb61-451f-ab2f-560b4eeccc49",
   "metadata": {},
   "source": [
    "### Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663e3e6-4389-48c9-9e5d-db7bdb922cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = []\n",
    "p_val_p = []\n",
    "colnew_p =[] #create empty lists to save the data\n",
    "for column in df1.columns: #iterate in our reference list\n",
    "        x = df12_merged.filter(like=column) #filter from the combine dataset only the columns that start with the \"name\" at a time\n",
    "        x = x.dropna() #drop the NaN for the specified duplet\n",
    "        if len(x) > 2: # setting a threshold of at least 3 values\n",
    "                r, p = stats.pearsonr(x[x.columns[0]], x[x.columns[1]]) #Pearson correlation\n",
    "                corr.append (r)\n",
    "                p_val_p.append (p)\n",
    "                colnew_p.append (column) # add the specified values to the list\n",
    "res_per1 = pd.DataFrame (corr, columns=[\"Pearson_correlation_value\"])\n",
    "res_per2 = pd.DataFrame (p_val_p, columns=[\"P-value(Pear-corr)\"])\n",
    "res_per3 = pd.DataFrame (colnew_p, columns=[\"Name\"]) # create df from lists\n",
    "per_corr = pd.concat([res_per1,res_per2,res_per3], axis =1) #merge into one\n",
    "per_corr #visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259b57d-f0a5-4624-9678-5eb94d85a9d2",
   "metadata": {},
   "source": [
    "### Student t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78681d18-ed82-4c1b-9f66-08f8148f02fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stut = []\n",
    "p_val_s = []\n",
    "colnew_s =[]#create empty lists to save the data\n",
    "for column in df1.columns: #iterate in our reference list\n",
    "        x = df12_merged.filter(like=column) #filter from the combine dataset only the columns that start with the \"name\" at a time\n",
    "        x = x.dropna() #drop the NaN for the specified duplet\n",
    "        if len(x) > 2: # setting a threshold of at least 3 values\n",
    "                r, p = stats.ttest_rel(x[x.columns[0]], x[x.columns[1]]) #paired test\n",
    "                stut.append (r)\n",
    "                p_val_s.append (p)\n",
    "                colnew_s.append (column)# add the specified values to the list\n",
    "res_stut1 = pd.DataFrame (stut, columns=[\"T-test_value\"])\n",
    "res_stut2 = pd.DataFrame (p_val_s, columns=[\"P-value(T-Test)\"])\n",
    "res_stut3 = pd.DataFrame (colnew_s, columns=[\"Name\"])# create df from lists\n",
    "t_test = pd.concat([res_stut1,res_stut2,res_stut3], axis =1) #merge into one\n",
    "t_test #visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187eaa8a-0866-451b-9e7a-140b7db89f7f",
   "metadata": {},
   "source": [
    "### Wilcoxon Rank test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ab147-4d02-472b-9d49-19242d887b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "wilc = []\n",
    "p_val_w = []\n",
    "colnew_w =[]#create empty lists to save the data\n",
    "for column in df1.columns: #iterate in our reference list\n",
    "        x = df12_merged.filter(like=column) #filter from the combine dataset only the columns that start with the \"name\" at a time\n",
    "        x = x.dropna() #drop the NaN for the specified duplet\n",
    "        if len(x) > 2: # setting a threshold of at least 3 values\n",
    "            r, p = stats.wilcoxon(x[x.columns[0]], x[x.columns[1]], alternative='two-sided') #wilcoxon test\n",
    "            wilc.append (r)\n",
    "            p_val_w.append (p)\n",
    "            colnew_w.append (column)# add the specified values to the list\n",
    "res_wilc1 = pd.DataFrame (wilc, columns=[\"Wilconxon_stats\"])\n",
    "res_wilc2 = pd.DataFrame (p_val_w, columns=[\"P-value(Wilcoxon)\"])\n",
    "res_wilc3 = pd.DataFrame (colnew_w, columns=[\"Name\"])# create df from lists\n",
    "wilc_rank = pd.concat([res_wilc1,res_wilc2,res_wilc3], axis =1) #merge into one\n",
    "wilc_rank #visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70b5c7-75d3-455c-b902-05e8046d8ca4",
   "metadata": {},
   "source": [
    "## Create a comparison complete table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38515e-1a31-42db-b0c6-e4e83fe6f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_corr.set_index(\"Name\", inplace=True)\n",
    "t_test.set_index(\"Name\", inplace=True) # set the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2137c07a-c082-4348-9367-e03b09e137f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_corr#visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327db659-541f-40bf-b4f6-97c28c54eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test #visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8557fe8b-07fd-4612-804a-6d49e23e3e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff #visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862b9fa-a8c9-402b-938a-4161894c0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_t_compared = pd.merge(per_corr, t_test, how= \"outer\", on=\"Name\") \n",
    "#merge pearson df's using the combine column on name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432224e3-df24-4071-b1c3-79c1b19b1a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_t_compared #visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24ee99-d43f-4c1b-83ff-7e187207e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_wilc_compared = pd.merge(per_t_compared, mean_diff, how= \"outer\", on=\"Name\") \n",
    "#merge pearson df's using the combine column on name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e597f559-06af-43d7-96df-669f3f853496",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_compared = pd.merge(non_wilc_compared, wilc_rank, how= \"outer\", on=\"Name\") \n",
    "#merge pearson df's using the combine column on name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f4e48-207e-4332-bc93-ab4d0872ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_compared #visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b97ba03-a497-45b4-8b1a-44a8a69fee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_compared.to_csv(\"path...comparison_table_ALPS.csv\", decimal=',', sep=';', index = False)\n",
    "#save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c87dff-6da1-45b4-933d-f90e4af07eb2",
   "metadata": {},
   "source": [
    "### Bland-Altman Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e7037-45d0-4866-951b-b963d0ec8181",
   "metadata": {},
   "outputs": [],
   "source": [
    "font1 = {'family':'serif','color':'blue','size':20}\n",
    "\n",
    "for column in df1.columns:\n",
    "    d = df12_merged.filter(like=column)  #filter from the combine dataset only the columns that start with the \"name\" at a time\n",
    "    d = d.dropna() #drop the NaN for the specified duplet\n",
    "    if len(d) > 2:    # setting a threshold of at least 3 values\n",
    "        #create Bland-Altman plot                  \n",
    "        f, ax = plt.subplots(1, figsize = (8,5))\n",
    "        sm.graphics.mean_diff_plot(d[d.columns[0]], d[d.columns[1]], ax = ax)        \n",
    "        plt.title(column, fontdict = font1)\n",
    "        #display Bland-Altman plot\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
